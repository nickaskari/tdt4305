{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3408,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser  # for reading the parameters file\n",
    "import sys  # for system errors and printouts\n",
    "from pathlib import Path  # for paths of files\n",
    "import os  # for reading the input data\n",
    "import time  # for timing\n",
    "import numpy as np  # for creating matrices or arrays\n",
    "import random  # for randomly generating a and b for hash functions\n",
    "from itertools import combinations  # for creating candidate pairs in lsh\n",
    "import re\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3409,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_file = 'default_parameters.ini'  # the main parameters file\n",
    "data_main_directory = Path('data')\n",
    "parameters_dictionary = dict()\n",
    "document_list = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_prime(N):\n",
    "    def is_prime(n):\n",
    "        if n <= 2:\n",
    "            return n == 2\n",
    "        if n % 2 == 0:\n",
    "            return False\n",
    "        p = 3\n",
    "        while p * p <= n:\n",
    "            if n % p == 0:\n",
    "                return False\n",
    "            p += 2\n",
    "        return True\n",
    "\n",
    "    prime = N + 1\n",
    "    while not is_prime(prime):\n",
    "        prime += 1\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parameters():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(parameter_file)\n",
    "    for section in config.sections():\n",
    "        for key in config[section]:\n",
    "            if key == 'data':\n",
    "                parameters_dictionary[key] = config[section][key]\n",
    "            elif key == 'naive':\n",
    "                parameters_dictionary[key] = bool(config[section][key])\n",
    "            elif key == 't':\n",
    "                parameters_dictionary[key] = float(config[section][key])\n",
    "            else:\n",
    "                parameters_dictionary[key] = int(config[section][key])\n",
    "\n",
    "\n",
    "read_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    for (root, dirs, file) in os.walk(data_path):\n",
    "        for f in file:\n",
    "            file_path = data_path / f\n",
    "            doc = open(file_path).read().strip().replace('\\n', ' ')\n",
    "            file_id = int(file_path.stem)\n",
    "            document_list[file_id] = doc\n",
    "\n",
    "\n",
    "data_folder = data_main_directory / parameters_dictionary['data']\n",
    "read_data(data_folder)\n",
    "document_list = {k: document_list[k] for k in sorted(document_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the toy data to test, remove stop words as in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3413,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_1 = \" Big Data platform  students  Blackboard\"\n",
    "Sentence_2 = \"Questions  MinHash project NTNU students Piazza\"\n",
    "Sentence_3 = \"NTNU Big Data platform  Blackboard  Piazza\"\n",
    "Sentence_4 = \" project data  students   Blackboard  Piazza\"\n",
    "\n",
    "Sentence_1, Sentence_2, Sentence_3, Sentence_4 = [sentence.lower()\n",
    "                                                  for sentence in [Sentence_1, Sentence_2, Sentence_3, Sentence_4]]\n",
    "\n",
    "document_list = {1: Sentence_1, 2: Sentence_2, 3: Sentence_3, 4: Sentence_4}\n",
    "\n",
    "shingles = ['big', 'blackboard', 'data', 'minhash', 'ntnu',\n",
    "                'piazza', 'platform', 'project', 'questions', 'students']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :   big data platform  students  blackboard\n",
      "2 :  questions  minhash project ntnu students piazza\n",
      "3 :  ntnu big data platform  blackboard  piazza\n",
      "4 :   project data  students   blackboard  piazza\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(document_list)):\n",
    "    print(i + 1, ': ', document_list[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKS!\n",
    "\n",
    "def k_shingles():\n",
    "    docs_k_shingles = []\n",
    "    #k = parameters_dictionary['k']\n",
    "    k = 1\n",
    "    non_word_pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "    for doc_id, document in document_list.items():\n",
    "        cleaned_doc = re.sub(non_word_pattern, '', document)\n",
    "        words = cleaned_doc.split()\n",
    "        k_shingles_set = set([' '.join(words[i:i+k]) for i in range(len(words) - k + 1)])\n",
    "        docs_k_shingles.append(k_shingles_set)\n",
    "\n",
    "    return docs_k_shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'platform', 'students', 'data', 'big', 'blackboard'}\n",
      "2 :  {'students', 'minhash', 'piazza', 'project', 'ntnu', 'questions'}\n",
      "3 :  {'platform', 'data', 'piazza', 'ntnu', 'big', 'blackboard'}\n",
      "4 :  {'students', 'piazza', 'data', 'project', 'blackboard'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'big', 'blackboard', 'data', 'platform', 'students'},\n",
       " {'minhash', 'ntnu', 'piazza', 'project', 'questions', 'students'},\n",
       " {'big', 'blackboard', 'data', 'ntnu', 'piazza', 'platform'},\n",
       " {'blackboard', 'data', 'piazza', 'project', 'students'}]"
      ]
     },
     "execution_count": 3416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs_k_shingles = k_shingles()\n",
    "for i in range(len(all_docs_k_shingles)):\n",
    "    print(i + 1, ': ', all_docs_k_shingles[i])\n",
    "\n",
    "\n",
    "all_docs_k_shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_set(k_shingles):\n",
    "\n",
    "    all_unique_shingles = set().union(*k_shingles)  # can add *k_shingles instead\n",
    "    all_unique_shingles_list = list(all_unique_shingles)\n",
    "\n",
    "    shingle_to_index = {shingle: idx for idx,\n",
    "                        shingle in enumerate(all_unique_shingles_list)}\n",
    "\n",
    "    num_docs = len(k_shingles)\n",
    "    num_shingles = len(all_unique_shingles)\n",
    "    input_matrix = np.zeros((num_shingles, num_docs), dtype=int)\n",
    "\n",
    "    for doc_idx, shingles_set in enumerate(k_shingles):\n",
    "        for shingle in shingles_set:\n",
    "            shingle_idx = shingle_to_index[shingle]\n",
    "            input_matrix[shingle_idx, doc_idx] = 1\n",
    "\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0],\n",
       "       [1, 1, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 1, 1],\n",
       "       [0, 1, 1, 1],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 1, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [1, 0, 1, 1]])"
      ]
     },
     "execution_count": 3418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix = signature_set(all_docs_k_shingles)\n",
    "input_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test (above) = actual (in task 1) \n",
    "\n",
    "1 = 1 \n",
    "\n",
    "2 = 10\n",
    "\n",
    "\n",
    "3 = 4\n",
    "\n",
    "\n",
    "4 = 3\n",
    "\n",
    "\n",
    "5 = 6\n",
    "\n",
    "\n",
    "6 = 8\n",
    "\n",
    "\n",
    "7 = 5\n",
    "\n",
    "\n",
    "8 = 9\n",
    "\n",
    "\n",
    "9 = 7\n",
    "\n",
    "\n",
    "10 = 2\n",
    "\n",
    "I.e all rows (shingles / Unique words) are present, but in a different order (not alphabetially sorted). OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 3419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix.shape[0], input_matrix.shape[1]\n",
    "# Num Shingles X Num Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hash_functions(num_perm, N):\n",
    "    hash_funcs = []\n",
    "    for i in range(1, num_perm + 1):\n",
    "        a = random.randint(1, N)  \n",
    "        b = random.randint(0, N)  \n",
    "        p = next_prime(N)     \n",
    "        hash_func = (lambda x, a=a, b=b, p=p: ((a * x + b) % (p)) + 1, {'a': a, 'b': b, 'p': p})\n",
    "        hash_funcs.append(hash_func)\n",
    "    return hash_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minHash(docs_signature_sets, hash_fn):\n",
    "    input_matrix = docs_signature_sets  # for simplicity\n",
    "\n",
    "    num_shingles = input_matrix.shape[0]  # num rows\n",
    "    num_docs = input_matrix.shape[1]  # num columns\n",
    "    num_permutation = len(hash_fn)\n",
    "    min_hash_signatures = np.full((num_permutation, num_docs), np.inf)\n",
    "\n",
    "    for shingle in range(num_shingles):  # for each shingle, row\n",
    "        for doc in range(num_docs):  # for each doc, column\n",
    "            if input_matrix[shingle, doc] == 1:  \n",
    "                for permutation, (hash_func, params) in enumerate(hash_fn):\n",
    "                    shingle_hash = hash_func(shingle, **params)\n",
    "                    min_hash_signatures[permutation, doc] = min(\n",
    "                        min_hash_signatures[permutation, doc], shingle_hash)\n",
    "\n",
    "    return min_hash_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef minHash(docs_signature_sets, hash_fn):\\n    input_matrix = docs_signature_sets # simplicity\\n    num_shingles = input_matrix.shape[0]  # num rows\\n    num_docs = input_matrix.shape[1]  # num columns\\n    num_permutation = len(hash_fn)\\n    min_hash_signatures = np.full((num_permutation, num_docs), np.inf)\\n\\n    for permutation, (hash_func, params) in enumerate(hash_fn):\\n        print(\\n            f\"Hash function #{permutation}: a={params[\\'a\\']}, b={params[\\'b\\']}, p={params[\\'p\\']}\")\\n        for doc in range(num_docs): # for each doc, column\\n            for shingle in range(num_shingles): # for each shingle\\n                if input_matrix[shingle, doc] == 1:                    \\n                    shingle_hash = hash_func(shingle)\\n                    min_hash_signatures[permutation, doc] = min(\\n                        min_hash_signatures[permutation, doc], shingle_hash)\\n\\n    return min_hash_signatures\\n    '"
      ]
     },
     "execution_count": 3422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A more intuitiv implementation, but too slow\n",
    "\n",
    "def minHash(docs_signature_sets, hash_fn):\n",
    "    input_matrix = docs_signature_sets # simplicity\n",
    "    num_shingles = input_matrix.shape[0]  # num rows\n",
    "    num_docs = input_matrix.shape[1]  # num columns\n",
    "    num_permutation = len(hash_fn)\n",
    "    min_hash_signatures = np.full((num_permutation, num_docs), np.inf)\n",
    "\n",
    "    for permutation, (hash_func, params) in enumerate(hash_fn):\n",
    "        print(\n",
    "            f\"Hash function #{permutation}: a={params['a']}, b={params['b']}, p={params['p']}\")\n",
    "        for doc in range(num_docs): # for each doc, column\n",
    "            for shingle in range(num_shingles): # for each shingle\n",
    "                if input_matrix[shingle, doc] == 1:                    \n",
    "                    shingle_hash = hash_func(shingle)\n",
    "                    min_hash_signatures[permutation, doc] = min(\n",
    "                        min_hash_signatures[permutation, doc], shingle_hash)\n",
    "\n",
    "    return min_hash_signatures\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash_fn = generate_hash_functions(parameters_dictionary['permutations'], len(input_matrix))\n",
    "hash_fn = generate_hash_functions(3, len(input_matrix))\n",
    "min_hash_signatures = minHash(input_matrix, hash_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 1., 5., 1.],\n",
       "       [2., 1., 3., 1.],\n",
       "       [1., 2., 1., 2.]])"
      ]
     },
     "execution_count": 3424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permutations x Num_Documents\n",
    "min_hash_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3425,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_jaccard_similarity = [0.1, 0.5714, 0.4285, 0.2, 0.375, 0.375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity between Sentence 1 and Sentence 2 is 0.0\n",
      "Jaccard Similarity between Sentence 1 and Sentence 3 is 0.3333333333333333\n",
      "Jaccard Similarity between Sentence 1 and Sentence 4 is 0.0\n",
      "Jaccard Similarity between Sentence 2 and Sentence 3 is 0.0\n",
      "Jaccard Similarity between Sentence 2 and Sentence 4 is 1.0\n",
      "Jaccard Similarity between Sentence 3 and Sentence 4 is 0.0\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(matrix, i, j):\n",
    "    return np.sum(matrix[:, i] == matrix[:, j]) / matrix.shape[0]\n",
    "\n",
    "'''\n",
    "If we let the number og permutations grow large, the error from true similarity decreases. I.e., it works \n",
    "However, its a bit trash, need like 1000 permuatations for the result to be decent. And 1000 rows is more\n",
    "than the original, and there is no computational gain, but this may be because our data is so small\n",
    "'''\n",
    "\n",
    "for i in range(min_hash_signatures.shape[1]):\n",
    "    for j in range(i + 1, min_hash_signatures.shape[1]):\n",
    "        print(\n",
    "            f\"Jaccard Similarity between Sentence {i + 1} and Sentence {j + 1} is {jaccard_similarity(min_hash_signatures, i, j)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(min_hash_signatures):\n",
    "    \n",
    "    b = parameters_dictionary['b']\n",
    "    num_rows, num_docs = min_hash_signatures.shape\n",
    "    rows_per_band = num_rows // b\n",
    "    candidates = set()\n",
    "\n",
    "    for band in range(b):\n",
    "        start_index = band * rows_per_band\n",
    "        end_index = start_index + rows_per_band\n",
    "        buckets = {}\n",
    "\n",
    "        for doc in range(num_docs):            \n",
    "            \n",
    "            band_slice = tuple(min_hash_signatures[start_index:end_index, doc])\n",
    "\n",
    "            band_hash = hash(band_slice)\n",
    "\n",
    "            if band_hash not in buckets:\n",
    "                buckets[band_hash] = [doc]\n",
    "            else:\n",
    "                for candidate_doc in buckets[band_hash]:\n",
    "                    candidates.add((candidate_doc, doc))\n",
    "                buckets[band_hash].append(doc)\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)}"
      ]
     },
     "execution_count": 3428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = lsh(min_hash_signatures)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_similarities(candidate_docs, min_hash_matrix):\n",
    "    similarity_dict = {}\n",
    "    #t = parameters_dictionary['t']\n",
    "    t = 0.5\n",
    "\n",
    "    for candidate_pair in candidate_docs:\n",
    "\n",
    "        doc1, doc2 = list(candidate_pair)\n",
    "\n",
    "        agreement = np.sum(\n",
    "            min_hash_matrix[:, doc1] == min_hash_matrix[:, doc2])\n",
    "\n",
    "        similarity = agreement / min_hash_matrix.shape[0]\n",
    "\n",
    "        if similarity > t:\n",
    "            similarity_dict[candidate_pair] = similarity\n",
    "\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 3): 1.0}"
      ]
     },
     "execution_count": 3430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = candidates_similarities(candidates, min_hash_signatures)\n",
    "similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
