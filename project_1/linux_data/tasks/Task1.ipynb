{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d8965a-c0b4-4802-abb8-05fc5e21ce59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Names of people in the group\n",
    "\n",
    "Please write the names of the people in your group in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c623c5-86c0-48ee-85de-5d514b4b334f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nick Askari\n",
    "\n",
    "Simen Peder Stang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "324eb902-51f2-44b0-8ff1-730efac9900c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: True"
     ]
    }
   ],
   "source": [
    "# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n",
    "dbutils.fs.rm(\"/user\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b530d6-b580-4de2-affb-aad878f4da86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-4ebc8707-28c6-4b40-9385-2e7015623720/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n",
    "!pip install -q ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023f4d10-e729-450c-8d1a-50d494b488d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading PySpark modules that we need\n",
    "import unittest\n",
    "from collections import Counter\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcda919c-b51e-4b61-9d61-80cc24f2d15e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: defining the schema for the data\n",
    "Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8bdba5-f6d6-43bf-bed9-763d99cfcc91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining a schema for 'badges' table\n",
    "badges_schema = StructType([StructField('UserId', IntegerType(), False),\n",
    "                            StructField('Name', StringType(), False),\n",
    "                            StructField('Date', TimestampType(), False),\n",
    "                            StructField('Class', IntegerType(), False)])\n",
    "\n",
    "# Defining a schema for 'posts' table\n",
    "posts_schema = StructType([StructField('Id', IntegerType(), False),\n",
    "                           StructField('ParentId', IntegerType(), True), \n",
    "                           StructField('PostTypeId', IntegerType(), False),\n",
    "                           StructField('CreationDate', TimestampType(), False), \n",
    "                           StructField('Score', IntegerType(), False),\n",
    "                           StructField('ViewCount', IntegerType(), False),\n",
    "                           StructField('Body', StringType(), False), \n",
    "                           StructField('OwnerUserId', IntegerType(), False),\n",
    "                           StructField('LastActivityDate', TimestampType(), False),\n",
    "                           StructField('Title', StringType(), True),\n",
    "                           StructField('Tags', StringType(), True), \n",
    "                           StructField('AnswerCount', IntegerType(), True), \n",
    "                           StructField('CommentCount', IntegerType(), False),\n",
    "                           StructField('FavoriteCount', IntegerType(), True),\n",
    "                           StructField('CloseDate', TimestampType(), True)])\n",
    "## To-do!\n",
    "\n",
    "# Defining a schema for 'users' table\n",
    "users_schema = StructType([\n",
    "    StructField('Id', IntegerType(), False),  \n",
    "    StructField('Reputation', IntegerType(), False),\n",
    "    StructField('CreationDate', TimestampType(), False),\n",
    "    StructField('DisplayName', StringType(), False),\n",
    "    StructField('LastAccessDate', TimestampType(), False),\n",
    "    StructField('AboutMe', StringType(), True),\n",
    "    StructField('Views', IntegerType(), False),\n",
    "    StructField('UpVotes', IntegerType(), False),\n",
    "    StructField('DownVotes', IntegerType(), False)\n",
    "])\n",
    "## To-do!\n",
    "\n",
    "# Defining a schema for 'comments' table\n",
    "comments_schema = StructType([\n",
    "    StructField('PostId', IntegerType(), False),\n",
    "    StructField('Score', IntegerType(), False),\n",
    "    StructField('Text', StringType(), False),  \n",
    "    StructField('CreationDate', TimestampType(), False),\n",
    "    StructField('UserId', IntegerType(), False)\n",
    "])\n",
    "## To-do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: implementing two helper functions\n",
    "Next, we need to implement two helper functions:\n",
    "1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n",
    "2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n",
    "\n",
    "Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n",
    "\n",
    "BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089f87ff-f2c8-4ac8-8449-cec251c502f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def load_csv(source_file: \"path for the CSV file to load\", schema: \"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n",
    "    ## To-do!\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    return spark.read.csv(path=source_file, sep='\\t', schema=schema, header=True)\n",
    "\n",
    "def save_df(df: \"DataFrame to be saved\", table_name: \"name under which the DataFrame will be saved\") -> None:\n",
    "    ## To-do!\n",
    "    df.write.mode('overwrite').parquet(f\"dbfs:/user/hive/warehouse/{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39bc683c-b37a-4842-8bf8-004620b17cca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8903e903-7e4f-4c15-99bd-c9129a601fde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementation by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n",
    "\n",
    "Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd470d59-2571-4b7d-b022-9ee8f7c3e281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": ".....\n----------------------------------------------------------------------\nRan 5 tests in 25.960s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 25.960s\n",
      "\n",
      "OK\n",
      "Out[22]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"
     ]
    }
   ],
   "source": [
    "%%unittest_main\n",
    "class TestTask1(unittest.TestCase):\n",
    "   \n",
    "    # test 1\n",
    "    def test_load_badges(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n",
    "        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 2\n",
    "    def test_load_posts(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n",
    "        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n",
    "                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n",
    "                                    'CloseDate']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 3\n",
    "    def test_load_comments(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n",
    "        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    \n",
    "    # test 4\n",
    "    def test_load_users(self):\n",
    "        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n",
    "        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n",
    "        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n",
    "        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n",
    "\n",
    "        coulmn_names = Counter(map(str.lower,\n",
    "                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n",
    "                                    'Views', 'UpVotes', 'DownVotes']))\n",
    "        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n",
    "                              \"Missing column(s) or column name mismatch\")\n",
    "    # test 5\n",
    "    def test_save_dfs(self):\n",
    "        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n",
    "               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n",
    "               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n",
    "               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n",
    "               ]\n",
    "\n",
    "        for i in dfs:\n",
    "            df = load_csv(source_file=i[0], schema=i[1])\n",
    "            save_df(df, i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f99b257-8618-4796-aeb0-d9446863c259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n",
    "\n",
    "Write your descriptions in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ec9fda-7848-4f01-8a36-3b82b78be007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your descriptions...\n",
    "\n",
    "### Spark Application\n",
    "\n",
    "A Spark Application is a program which details how the Spark system should process data. To explain what exactly a spark application is, it is convinent to briefly mention what Apache Spark is. One can use Spark to facilitate processing and analyse huge amounts of data. The data is divided into different machines, or clusters. This means that Spark can distribute computation accross these different machines. In other words it is capable of parallell processing where it can process data in different nodes within a cluster. Hence, it is suitable when dealing with big data. \n",
    "\n",
    "Now the Spark Application is simply the user built program which details the data processsing. In such an application you could for example start by creating a Sparkcontext, which details where to access some given cluster. Then one can write code to perform transformations or action (more on this later). These applications can be written in Java, Scala, Python or R. \n",
    "\n",
    "### SparkSession\n",
    "\n",
    "SparkSession is the entry point to programming Spark with the Dataset and DataFrame API, as mentioned in the documentation. It is essentiallly an access point to all Spark functionalities. A SparkSession can be used to create DataFrames (like in this task), register DataFrames as tables, execute SQL queries, cache tables, and read data from various sources. \n",
    "\n",
    "### Transformations\n",
    "\n",
    "Transformations creates a new RDD (Resilient Distributed Dataset) from a previous one. A transformation could be an operation like 'filter', 'map' or 'join' to mention a few. One important fact about transformations is that they are computed 'lazily' which we will come back to.\n",
    "\n",
    "Another key fact is that once you perform transformations and create new RDD's, spark keeps track of the dependencies in a *lineage graph*. This way it knows how to compute each of the RDD's. In addition, if some data is lost, then one can recover data with this graph.\n",
    "\n",
    "### Actions\n",
    "\n",
    "With actions, as opposed to transformation, are operations that actually triggers some computations. Actions are such that they return a value to the driver program, or writing to external storage. In other words, output needs to be produced. Typical actions could be something like count(). Also bear in mind that an action might be performed on some transformation of a RDD, hence when the action is run, then the transformation also must be run (or evaluated).\n",
    "\n",
    "### Lazy Evaluation\n",
    "\n",
    "As mentioned, transformations are evaluated lazily. Hence, when you just write a line of code where you execute some evaluation, then it does not execute until it has to for some action. This way you kind of delay computation. Making the system more efficient, avoiding heavy computation until it really has to. Also one minimizes data movement, as it can prove to be costly. Spark accomplishes this be constructing a DAG (Directed Acyclic Graph). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabc5dd6-fc51-4699-9147-8336f98a6116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 647188828532740,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Task1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
